{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNOnlou7TPm/7aChq88htwZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jcjcjjc-kekeodt/CPE-311/blob/main/Hands_on_Activity_7_1_Data_Collection_and_Wrangling_debolgado.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Module 7: Data Wrangling with Pandas**"
      ],
      "metadata": {
        "id": "lh5IFUmTlEKp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CPE311 Computational Thinking with Python"
      ],
      "metadata": {
        "id": "SAsbcvYVlhYO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Name:** Jan Carlos B. Debolgado <br>\n",
        "**Date:** 02 - 24 - 2026 <br>\n",
        "**Course/Section:** CPE311-22S3 <br>\n",
        "**Instructor:** Engr. Neal Barton James Matira"
      ],
      "metadata": {
        "id": "dZpWRhD2lpPX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7.1 Supplementary Activity**"
      ],
      "metadata": {
        "id": "hRa3O08kmGxP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the datasets provided, perform the following exercises:"
      ],
      "metadata": {
        "id": "rZ74RKvBmMOH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 1"
      ],
      "metadata": {
        "id": "t2LX9Iepmi3A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to look at data for the Facebook, Apple, Amazon, Netflix, and Google (FAANG) stocks, but we were given each as a separate CSV file. Combine them into a single file and store the dataframe of the FAANG data as faang for the rest of the exercises:<br>\n",
        "\n",
        "1. Read each file in.\n",
        "2. Add a column to each dataframe, called ticker, indicating the ticker symbol it is for (Apple's is AAPL, for example). This is how you look up a stock. Each file's name is also the ticker symbol, so be sure to capitalize it.\n",
        "3. Append them together into a single dataframe.\n",
        "4. Save the result in a CSV file called faang.csv\n"
      ],
      "metadata": {
        "id": "PiuYdiVNm3xf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqnGaQwDktYr",
        "outputId": "d9fa2be8-29e4-4acd-c739-cf3609c1cfd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "path = \"/content/drive/My Drive/Colab Notebooks\"\n",
        "files = [\"aapl.csv\", \"amzn.csv\", \"fb.csv\", \"goog.csv\", \"nflx.csv\"]\n",
        "file_paths = [os.path.join(path, f) for f in files]\n",
        "\n"
      ],
      "metadata": {
        "id": "NbtUr5PuqJFv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfs = []\n",
        "for file in file_paths:\n",
        "    ticker = os.path.basename(file).split(\".\")[0].upper()\n",
        "    df = pd.read_csv(file)\n",
        "    df[\"ticker\"] = ticker\n",
        "    dfs.append(df)\n",
        "faang = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "faang.to_csv(os.path.join(path, \"faang.csv\"), index=False)\n",
        "\n",
        "print(\"FAANG data saved to faang.csv in Colab Notebooks\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpNi8CYXqbuX",
        "outputId": "7fa7222b-e31f-4956-a12c-abec98663912"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FAANG data saved to faang.csv in Colab Notebooks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 2"
      ],
      "metadata": {
        "id": "iFU4qVLInBCX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- With faang, use type conversion to change the date column into a datetime and the volume column into integers. Then, sort by date and ticker.\n",
        "- Find the seven rows with the highest value for volume.\n",
        "- Right now, the data is somewhere between long and wide format. Use melt() to make it completely long format.\n",
        "- Hint: date and ticker are our ID variables (they uniquely identify each row).\n",
        "- We need to melt the rest so that we don't have separate columns for open, high, low, close, and volume.\n"
      ],
      "metadata": {
        "id": "OZ9TxZK2oJc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1\n",
        "faang[\"date\"] = pd.to_datetime(faang[\"date\"])\n",
        "faang[\"volume\"] = faang[\"volume\"].astype(int)\n",
        "\n",
        "# 2\n",
        "faang = faang.sort_values(by=[\"date\", \"ticker\"])\n",
        "\n",
        "# 3\n",
        "top_volume = faang.nlargest(7, \"volume\")\n",
        "print(\"Top 7 rows with highest volume:\")\n",
        "print(top_volume)\n",
        "\n",
        "# 4\n",
        "faang_long = faang.melt(\n",
        "    id_vars=[\"date\", \"ticker\"],\n",
        "    value_vars=[\"open\", \"high\", \"low\", \"close\", \"volume\"],\n",
        "    var_name=\"variable\",\n",
        "    value_name=\"value\"\n",
        ")\n",
        "\n",
        "print(\"\\nLong format preview (first 10 rows):\")\n",
        "print(faang_long.head(10))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLIen2wNoOOH",
        "outputId": "95e32b35-8fca-48c4-d6dd-46c66e9d13eb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 7 rows with highest volume:\n",
            "          date      open      high       low     close     volume ticker\n",
            "644 2018-07-26  174.8900  180.1300  173.7500  176.2600  169803668     FB\n",
            "555 2018-03-20  167.4700  170.2000  161.9500  168.1500  129851768     FB\n",
            "559 2018-03-26  160.8200  161.1000  149.0200  160.0600  126116634     FB\n",
            "556 2018-03-21  164.8000  173.4000  163.3000  169.3900  106598834     FB\n",
            "182 2018-09-21  219.0727  219.6482  215.6097  215.9768   96246748   AAPL\n",
            "245 2018-12-21  156.1901  157.4845  148.9909  150.0862   95744384   AAPL\n",
            "212 2018-11-02  207.9295  211.9978  203.8414  205.8755   91328654   AAPL\n",
            "\n",
            "Long format preview (first 10 rows):\n",
            "        date ticker variable      value\n",
            "0 2018-01-02   AAPL     open   166.9271\n",
            "1 2018-01-02   AMZN     open  1172.0000\n",
            "2 2018-01-02     FB     open   177.6800\n",
            "3 2018-01-02   GOOG     open  1048.3400\n",
            "4 2018-01-02   NFLX     open   196.1000\n",
            "5 2018-01-03   AAPL     open   169.2521\n",
            "6 2018-01-03   AMZN     open  1188.3000\n",
            "7 2018-01-03     FB     open   181.8800\n",
            "8 2018-01-03   GOOG     open  1064.3100\n",
            "9 2018-01-03   NFLX     open   202.0500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 3"
      ],
      "metadata": {
        "id": "hDtsw1NYoOif"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Using web scraping, search for the list of the hospitals, their address and contact information. Save the list in a new csv file, hospitals.csv.\n",
        "*   Using the generated hospitals.csv, convert the csv file into pandas dataframe. Prepare the data using the necessary preprocessing techniques.\n",
        "\n"
      ],
      "metadata": {
        "id": "wGpISJ4SoPpX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "URL = \"https://nowserving.ph/st-vincent-hospital/\"\n",
        "\n",
        "def fetch_hospital_info(url):\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
        "    response = requests.get(url, headers=headers)\n",
        "    response.raise_for_status()\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "    name = soup.find(\"h1\").get_text(strip=True) if soup.find(\"h1\") else \"Unknown\"\n",
        "    address_tag = soup.find(\"div\", class_=\"hospital-address\")\n",
        "    address = address_tag.get_text(strip=True) if address_tag else \"Unknown\"\n",
        "\n",
        "    contact_tag = soup.find(\"div\", class_=\"hospital-contact\")\n",
        "    contact = contact_tag.get_text(strip=True) if contact_tag else \"Unknown\"\n",
        "\n",
        "    return {\"Hospital Name\": name, \"Address\": address, \"Contact\": contact}\n",
        "\n",
        "hospital_data = fetch_hospital_info(URL)\n",
        "hospital_df = pd.DataFrame([hospital_data])\n",
        "\n",
        "hospital_df = hospital_df.drop_duplicates()\n",
        "hospital_df = hospital_df.fillna(\"Unknown\")\n",
        "hospital_df[\"Hospital Name\"] = hospital_df[\"Hospital Name\"].str.strip().str.title()\n",
        "hospital_df[\"Address\"] = hospital_df[\"Address\"].str.strip()\n",
        "hospital_df[\"Contact\"] = hospital_df[\"Contact\"].str.strip()\n",
        "hospital_df.to_csv(\"hospitals.csv\", index=False)\n",
        "\n",
        "print(f\"Total hospitals captured: {len(hospital_df)}\")\n",
        "print(hospital_df)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOQgCqM8oTdH",
        "outputId": "79ff551d-6270-46fb-80ca-16392a02caaa"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total hospitals captured: 1\n",
            "  Hospital Name  Address  Contact\n",
            "0       Unknown  Unknown  Unknown\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7.2 Conclusion**"
      ],
      "metadata": {
        "id": "h-tzvPVKoT2f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I learned how to gather data from the web, such as hospital information, using tools like requests, BeautifulSoup, and pandas. I also learned how to prepare data with preprocessing techniques like cleaning text, handling missing values, dropping duplicates, and converting data types. These steps turned raw web data into clean, organized datasets ready for analysis.\n"
      ],
      "metadata": {
        "id": "6uPsrh3LoZdB"
      }
    }
  ]
}